class DataFrameValidator:
    def __init__(self, schema_dict: dict):
        self.schema_dict = schema_dict
        self.pydantic_model = self.create_pydantic_model(self.convert_schema(schema_dict))

    def convert_schema(self, schema_dict):
        converted_schema = {}
        for key, value in schema_dict.items():
            if value == 'string':
                converted_schema[key] = (str, ...)
            elif value == 'int':
                converted_schema[key] = (int, ...)
            elif value == 'email':
                converted_schema[key] = (EmailStr, ...)
            elif value == 'date':
                converted_schema[key] = (date, ...)
            elif value == 'datetime':
                converted_schema[key] = (datetime, ...)
            elif value == 'decimal':
                converted_schema[key] = (Decimal, ...)
            elif value == 'float':
                converted_schema[key] = (float, ...)
        return converted_schema

    def create_pydantic_model(self, schema: dict) -> Type[BaseModel]:
        return create_model('DynamicModel', **schema)

    def parse_and_validate(self, df: pd.DataFrame) -> pd.DataFrame:
        def _parse_row(row):
            parsed_row = {}
            for field, value in row.items():
                if field in self.schema_dict:
                    field_type = self.schema_dict[field]
                    if value is None:
                        # Replace null values with default values based on the field type
                        if field_type == 'int':
                            parsed_row[field] = 0
                        elif field_type == 'float':
                            parsed_row[field] = 0.0
                        elif field_type == 'email':
                            parsed_row[field] = 'example@gmail.com'
                        elif field_type == 'date':
                            parsed_row[field] = '1999-06-06'
                        elif field_type == 'datetime' or field_type == 'timestamp':
                            parsed_row[field] = '1999-06-06'
                    else:
                        if field_type == 'date':
                            try:
                                parsed_row[field] = parser.parse(value).date().isoformat()
                            except ValueError:
                                parsed_row[field] = value  # Retain the original invalid value
                        elif field_type == 'datetime':
                            try:
                                parsed_row[field] = parser.parse(value)
                            except ValueError:
                                parsed_row[field] = value  # Retain the original invalid value
                        elif field_type == 'timestamp':
                            try:
                                parsed_row[field] = datetime.fromtimestamp(float(value))
                            except ValueError:
                                parsed_row[field] = value  # Retain the original invalid value
                        elif field_type == 'int':
                            value = int(round(float(value)))
                            parsed_row[field] = value
                        else:
                            parsed_row[field] = value
            return self.pydantic_model(**parsed_row).dict()

        validated_data = []
        for _, row in df.iterrows():
            try:
                validated_data.append(_parse_row(row))
            except ValueError as e:
                print(f"Validation failed for row with values {row.values}: {str(e)}")

        return pd.DataFrame(validated_data)

    def generate_fugue_schema(self, schema: dict) -> str:
        type_mapping = {
            'string': 'str',
            'int': 'int',
            'float': 'float',
            'email': 'str',
            'date': 'date',
            'datetime': 'datetime',
            'timestamp': 'timestamp',
        }
        return ", ".join([f"{field}:{type_mapping[field_type]}" for field, field_type in schema.items()])

    def validate_with_fugue(self, df: pd.DataFrame) -> pd.DataFrame:
        fugue_schema = self.generate_fugue_schema(self.schema_dict)
        return transform(df, self.parse_and_validate, schema=fugue_schema)


import logging
from concurrent.futures import ThreadPoolExecutor
import threading
import time
import queue
import shutil

class CustomLogger:
    def __init__(self, log_file, backup_log_file):
        self.log_file = log_file
        self.backup_log_file = backup_log_file
        self.logger = self.get_logger(log_file)
        self.log_queue = queue.Queue()
        self.log_thread = threading.Thread(target=self.log_writer)
        self.log_thread.start()

    def get_logger(self, log_file):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        return logger

    def log_writer(self):
        while True:
            level, message = self.log_queue.get()
            if message is None:  # Sentinel to shut down the logging thread
                self.log_queue.task_done()
                break
            self.logger.log(level, message)
            self.log_queue.task_done()

    def log(self, message, level=logging.INFO):
        self.log_queue.put((level, message))

    def stop(self):
        self.log_queue.put((None, None))
        self.log_thread.join()

    def save_logs_to_backup(self):
        try:
            shutil.copy(self.log_file, self.backup_log_file)
            print(f"Log file {self.log_file} successfully backed up to {self.backup_log_file}")
        except FileNotFoundError:
            print(f"File {self.log_file} was not found")
        except IOError as e:
            print(f"Error copying file {self.log_file} to {self.backup_log_file}: {e}")

# Create an instance of CustomLogger
logger = CustomLogger('logfile.log', 'backup_logfile.log')

def sub_process(item, log_messages):
    log_messages.append((logging.ERROR, f"DIGVIJAY happening {item}"))
    return log_messages

def process_item(item):
    thread_name = threading.current_thread().name
    log_messages = []

    def log(message, level=logging.INFO):
        log_messages.append((level, f"{thread_name} - {message}"))

    log(f"Processing item: {item}")
    time.sleep(1)
    log(f"IN processing item: {item}", logging.WARNING)
    log_messages = sub_process(item, log_messages)
    log(f"Finished processing item: {item}")

    for level, message in log_messages:
        logger.log(message, level)

items = [1, 4, 3, 5]

with ThreadPoolExecutor(max_workers=3) as executor:
    executor.map(process_item, items)

logger.stop()
logger.save_logs_to_backup()
logger.log("All items have been processed.")

