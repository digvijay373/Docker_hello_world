class DataFrameValidator:
    def __init__(self, schema_dict: dict):
        self.schema_dict = schema_dict
        self.pydantic_model = self.create_pydantic_model(self.convert_schema(schema_dict))

    def convert_schema(self, schema_dict):
        converted_schema = {}
        for key, value in schema_dict.items():
            if value == 'string':
                converted_schema[key] = (str, ...)
            elif value == 'int':
                converted_schema[key] = (int, ...)
            elif value == 'email':
                converted_schema[key] = (EmailStr, ...)
            elif value == 'date':
                converted_schema[key] = (date, ...)
            elif value == 'datetime':
                converted_schema[key] = (datetime, ...)
            elif value == 'decimal':
                converted_schema[key] = (Decimal, ...)
            elif value == 'float':
                converted_schema[key] = (float, ...)
        return converted_schema

    def create_pydantic_model(self, schema: dict) -> Type[BaseModel]:
        return create_model('DynamicModel', **schema)

    def parse_and_validate(self, df: pd.DataFrame) -> pd.DataFrame:
        def _parse_row(row):
            parsed_row = {}
            for field, value in row.items():
                if field in self.schema_dict:
                    field_type = self.schema_dict[field]
                    if value is None:
                        # Replace null values with default values based on the field type
                        if field_type == 'int':
                            parsed_row[field] = 0
                        elif field_type == 'float':
                            parsed_row[field] = 0.0
                        elif field_type == 'email':
                            parsed_row[field] = 'example@gmail.com'
                        elif field_type == 'date':
                            parsed_row[field] = '1999-06-06'
                        elif field_type == 'datetime' or field_type == 'timestamp':
                            parsed_row[field] = '1999-06-06'
                    else:
                        if field_type == 'date':
                            try:
                                parsed_row[field] = parser.parse(value).date().isoformat()
                            except ValueError:
                                parsed_row[field] = value  # Retain the original invalid value
                        elif field_type == 'datetime':
                            try:
                                parsed_row[field] = parser.parse(value)
                            except ValueError:
                                parsed_row[field] = value  # Retain the original invalid value
                        elif field_type == 'timestamp':
                            try:
                                parsed_row[field] = datetime.fromtimestamp(float(value))
                            except ValueError:
                                parsed_row[field] = value  # Retain the original invalid value
                        elif field_type == 'int':
                            value = int(round(float(value)))
                            parsed_row[field] = value
                        else:
                            parsed_row[field] = value
            return self.pydantic_model(**parsed_row).dict()

        validated_data = []
        for _, row in df.iterrows():
            try:
                validated_data.append(_parse_row(row))
            except ValueError as e:
                print(f"Validation failed for row with values {row.values}: {str(e)}")

        return pd.DataFrame(validated_data)

    def generate_fugue_schema(self, schema: dict) -> str:
        type_mapping = {
            'string': 'str',
            'int': 'int',
            'float': 'float',
            'email': 'str',
            'date': 'date',
            'datetime': 'datetime',
            'timestamp': 'timestamp',
        }
        return ", ".join([f"{field}:{type_mapping[field_type]}" for field, field_type in schema.items()])

    def validate_with_fugue(self, df: pd.DataFrame) -> pd.DataFrame:
        fugue_schema = self.generate_fugue_schema(self.schema_dict)
        return transform(df, self.parse_and_validate, schema=fugue_schema)


import logging
from concurrent.futures import ThreadPoolExecutor
import threading
import time
import queue
import shutil

class CustomLogger:
    def __init__(self, log_file, backup_log_file):
        self.log_file = log_file
        self.backup_log_file = backup_log_file
        self.logger = self.get_logger(log_file)
        self.log_queue = queue.Queue()
        self.log_thread = threading.Thread(target=self.log_writer)
        self.log_thread.start()

    def get_logger(self, log_file):
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        return logger

    def log_writer(self):
        while True:
            level, message = self.log_queue.get()
            if message is None:  # Sentinel to shut down the logging thread
                self.log_queue.task_done()
                break
            self.logger.log(level, message)
            self.log_queue.task_done()

    def log(self, message, level=logging.INFO):
        self.log_queue.put((level, message))

    def stop(self):
        self.log_queue.put((None, None))
        self.log_thread.join()

    def save_logs_to_backup(self):
        try:
            shutil.copy(self.log_file, self.backup_log_file)
            print(f"Log file {self.log_file} successfully backed up to {self.backup_log_file}")
        except FileNotFoundError:
            print(f"File {self.log_file} was not found")
        except IOError as e:
            print(f"Error copying file {self.log_file} to {self.backup_log_file}: {e}")

# Create an instance of CustomLogger
logger = CustomLogger('logfile.log', 'backup_logfile.log')

def sub_process(item, log_messages):
    log_messages.append((logging.ERROR, f"DIGVIJAY happening {item}"))
    return log_messages

def process_item(item):
    thread_name = threading.current_thread().name
    log_messages = []

    def log(message, level=logging.INFO):
        log_messages.append((level, f"{thread_name} - {message}"))

    log(f"Processing item: {item}")
    time.sleep(1)
    log(f"IN processing item: {item}", logging.WARNING)
    log_messages = sub_process(item, log_messages)
    log(f"Finished processing item: {item}")

    for level, message in log_messages:
        logger.log(message, level)

items = [1, 4, 3, 5]

with ThreadPoolExecutor(max_workers=3) as executor:
    executor.map(process_item, items)

logger.stop()
logger.save_logs_to_backup()
logger.log("All items have been processed.")



import boto3

# Initialize the S3 client
s3 = boto3.client('s3')

bucket_name = 'your-s3-bucket-name'
documents_prefix = 'Documents/'  # Root folder where claim PDFs are stored

# Function to list all PDF files in the 'Documents/' folder recursively
def poll_for_pdfs(bucket, prefix):
    pdf_files = []
    
    # Paginate through the S3 objects
    paginator = s3.get_paginator('list_objects_v2')
    response_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)

    for page in response_iterator:
        if 'Contents' in page:
            for obj in page['Contents']:
                key = obj['Key']
                if key.endswith('.pdf'):  # Only select PDF files
                    pdf_files.append(key)

    return pdf_files

# Call the function to retrieve the list of PDF file paths
pdf_files = poll_for_pdfs(bucket_name, documents_prefix)

# Print the list of found PDFs
for pdf in pdf_files:
    print(f"Found PDF: s3://{bucket_name}/{pdf}")

from IPython.display import display, IFrame

# Function to download and display a PDF file from S3
def display_pdf_from_s3(s3_bucket, s3_key):
    # Download the PDF file from S3 to a local file
    local_pdf_filename = s3_key.split('/')[-1]  # Extract just the file name
    s3.download_file(s3_bucket, s3_key, local_pdf_filename)

    # Display the PDF using IFrame
    display(IFrame(local_pdf_filename, width=600, height=800))

# Iterate through each PDF path, download and display the PDF
for pdf in pdf_files:
    print(f"Displaying PDF: {pdf}")
    display_pdf_from_s3(bucket_name, pdf)



def transform_multiple_dicts(data_list):
    """
    Transform a list of dictionaries by mapping keys from one part to values from another part.
    If there are two dictionaries, map 1st to 2nd. If there are three, map 1st to 3rd.
    
    Parameters:
    data_list (list): List of dictionaries each containing mapping and values dictionaries.
    
    Returns:
    dict: A new dictionary with keys mapped to corresponding values from the dictionaries.
    """
    result = {}
    
    for data in data_list:
        # Identify keys for mapping and values
        keys = list(data.keys())
        
        if len(keys) < 2 or len(keys) > 3:
            raise ValueError("Each dictionary must contain 2 or 3 keys for mapping and values.")
        
        # Always map 1st dictionary
        mapping_key = keys[0]
        # Map to 2nd if there are two dictionaries, otherwise map to the 3rd
        values_key = keys[2] if len(keys) == 3 else keys[1]
        
        # Extract mapping and values dictionaries
        mapping = data[mapping_key]
        values = data[values_key]
        
        # Update result with the current mapping
        result[mapping_key] = values_key
        result.update({mapping[i]: values[i] for i in mapping})
    
    return result

# Example usage
data_list = [
    {
        'Mapping Key': {0: 'key1', 1: 'key2', 2: 'key3'},
        'Values Key': {0: 'value1', 1: 'value2', 2: 'value3'},
        'Other Key': {0: 'valA', 1: 'valB', 2: 'valC'}
    }
]

# Transform the list of dictionaries
transformed = transform_multiple_dicts(data_list)
print(transformed)




def transform_with_prefixes(input_list, prefixes):
    transformed_dict = {}
    
    # Iterate over the input list and the prefixes
    for idx, data_dict in enumerate(input_list):
        # Determine the prefix based on the current index
        if idx < len(prefixes):
            prefix = prefixes[idx]
        else:
            continue  # Skip if there are more dictionaries than prefixes
        
        # Add entries to the transformed dictionary
        for i, value in data_dict.items():
            # Use index + 1 to start numbering from 1
            transformed_dict[f'{prefix}{i+1}'] = value
    
    return transformed_dict

# Example input
input_list = [
    {0: 'key4', 1: 'key5', 2: 'key6'},
    {0: 'val4', 1: 'val5', 2: 'val6'},
    {0: 'val234', 1: 'val5345', 2: 'val456'},
    {0: 'anotherKey', 1: 'anotherVal', 2: 'yetAnotherVal'}
]

# List of prefixes
prefixes = ['Part', 'Orientation', 'Source']

# Transform the dictionary
result = transform_with_prefixes(input_list, prefixes)

# Output the transformed dictionary
print(result)



def flatten_dict(input_dict):
    output_list = []
    
    # Iterate through each key-value pair in the input dictionary
    for key, value_dict in input_dict.items():
        # Create a new dictionary for the flattened data
        flattened_dict = {}
        
        # Start by adding the main key as the first item
        flattened_dict[0] = key
        
        # Add the nested values to the flattened dictionary
        for i, val in value_dict.items():
            flattened_dict[i + 1] = val  # Start numbering from 1
        
        # Append the flattened dictionary to the output list
        output_list.append(flattened_dict)
    
    return output_list

# Example input
input_dict = {
    'key4': {0: 'key5', 1: 'key6'},
    'val4': {0: 'val5', 1: 'val6'},
    'val234': {0: 'val5345', 1: 'val456'},
    'anotherKey': {0: 'anotherVal', 1: 'yetAnotherVal'}
}

# Flatten the dictionary
result = flatten_dict(input_dict)

# Output the flattened list of dictionaries
print(result)




import boto3
import psycopg2
import json
from datetime import datetime
from awsglue.context import GlueContext
from pyspark.context import SparkContext

# Initialize S3, Glue, and PostgreSQL connections
s3_client = boto3.client('s3')
sc = SparkContext()
glueContext = GlueContext(sc)

# S3 bucket and folder details
bucket_name = "your-bucket-name"
folder_name = "daily-files/"
checkpoint_folder = "checkpoint/"

# PostgreSQL connection details
pg_conn = psycopg2.connect(
    database="your_db", 
    user="your_user", 
    password="your_password", 
    host="your_host", 
    port="your_port"
)
pg_cursor = pg_conn.cursor()

# Get today's date in the format YYYY-MM-DD for checkpoint file
current_date = datetime.now().strftime('%Y-%m-%d')
checkpoint_file = f"s3://{bucket_name}/{checkpoint_folder}{current_date}.json"

# Function to load the checkpoint file for the current date
def load_checkpoint():
    try:
        obj = s3_client.get_object(Bucket=bucket_name, Key=f'{checkpoint_folder}{current_date}.json')
        processed_files = json.loads(obj['Body'].read().decode('utf-8'))["processed_files"]
    except s3_client.exceptions.NoSuchKey:
        # If no checkpoint file exists, return an empty list
        processed_files = []
    except Exception as e:
        processed_files = []
    return processed_files

# Function to create/update the checkpoint file for the current date in S3
def update_checkpoint(processed_files):
    new_checkpoint = {"processed_files": processed_files}
    s3_client.put_object(Body=json.dumps(new_checkpoint), Bucket=bucket_name, Key=f'{checkpoint_folder}{current_date}.json')

# Step 1: List files in the S3 bucket
response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_name)

# Step 2: Load checkpoint for the current date
processed_files = load_checkpoint()

# Step 3: Filter unprocessed files for today
for obj in response['Contents']:
    file_name = obj['Key'].split('/')[-1]  # Extract file name
    if file_name not in processed_files:
        # Step 4: Process the unprocessed file
        file_path = f"s3://{bucket_name}/{obj['Key']}"
        
        # Load data using Glue
        df = glueContext.create_dynamic_frame.from_options(
            connection_type="s3", 
            connection_options={"paths": [file_path]}, 
            format="csv"
        )
        
        # Convert to Spark DataFrame
        df_spark = df.toDF()

        # Insert data into PostgreSQL
        for row in df_spark.collect():
            insert_query = """INSERT INTO your_table (column1, column2, column3)
                              VALUES (%s, %s, %s)"""
            pg_cursor.execute(insert_query, (row['column1'], row['column2'], row['column3']))
        
        pg_conn.commit()

        # Step 5: Update the checkpoint after successful processing
        processed_files.append(file_name)
        update_checkpoint(processed_files)

# Close PostgreSQL connection
pg_cursor.close()
pg_conn.close()

